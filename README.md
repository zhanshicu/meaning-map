# meaning-map
Automatic estimation of meaning maps (Henderson and Hayes, 2017) by fine-tuning a multi-modal large language model (LLM) LLaVA.

Our ability to seemingly effortlessly focus attention on meaningful regions in complex scenes has long eluded automated analysis, requiring thousands of human judgments to map the semantic forces that guide our gaze. While meaning maps have emerged as powerful predictors of attention, at times surpassing traditional saliency models, their creation requires extensive human ratings that limit research scalability and reproducibility. Here we present an efficient computational approach for automated, real-time generation of meaning maps. We first demonstrate that a fine-tuned vision-language model (LLaVA) can predict human attention patterns with accuracy surpassing human raters.

<img width="962" alt="Screenshot 2025-01-12 at 09 50 07" src="https://github.com/user-attachments/assets/d60215bd-6e1c-41c5-b562-cab4d63a6a24" />

We assessed performance by measuring the Pearson correlation between generated meaning maps and attention maps from humans. Henderson and Hayes (2017) provide 40 photographs depicting real-world scenes, and their correponsing attention and meaning maps in [OSF](https://osf.io/ptsvm). The attention maps, derived from eye movement experiments, are the indicators of how human attention is distributed within real-world scenes and are considered the ground truth. The meaning maps estimated by human coders and generated by base LLaVA serve as the baselines for our comparison.

<img width="1234" alt="Screenshot 2025-01-12 at 09 55 44" src="https://github.com/user-attachments/assets/fdba07dd-23bf-4df9-8d0d-fe26321d374b" />

The fine-tuned LLaVA often matches or outperforms human annotations (gray dashed line), which demonstrates its capability to predict attention distributions within scenes with high accuracy.

## To implement:

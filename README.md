# Automated Meaning Maps for Attention Guidance Using Finetuned Large Language Models

Automatic estimation of meaning maps (Henderson and Hayes, 2017) by fine-tuning a multi-modal large language model (LLM) LLaVA.

Our ability to seemingly effortlessly focus attention on meaningful regions in complex scenes has long eluded automated analysis, requiring thousands of human judgments to map the semantic forces that guide our gaze. While meaning maps have emerged as powerful predictors of attention, at times surpassing traditional saliency models, their creation requires extensive human ratings that limit research scalability and reproducibility. Here we present an efficient computational approach for automated, real-time generation of meaning maps. We first demonstrate that a fine-tuned vision-language model (LLaVA) can predict human attention patterns with accuracy surpassing human raters.

<img width="962" alt="Screenshot 2025-01-12 at 09 50 07" src="https://github.com/user-attachments/assets/d60215bd-6e1c-41c5-b562-cab4d63a6a24" />

We assessed performance by measuring the Pearson correlation between generated meaning maps and attention maps from humans. Henderson and Hayes (2017) provide 40 photographs depicting real-world scenes, and their correponsing attention and meaning maps in [OSF](https://osf.io/ptsvm). The attention maps, derived from eye movement experiments, are the indicators of how human attention is distributed within real-world scenes and are considered the ground truth. The meaning maps estimated by human coders and generated by base LLaVA serve as the baselines for our comparison.

<img width="1234" alt="Screenshot 2025-01-12 at 09 55 44" src="https://github.com/user-attachments/assets/fdba07dd-23bf-4df9-8d0d-fe26321d374b" />

The fine-tuned LLaVA often matches or outperforms human annotations (gray dashed line), which demonstrates its capability to predict attention distributions within scenes with high accuracy.

**To implement:**

First, download the data set of 40 digitized photographs and their corresponding meaning maps from [OSF](https://osf.io/ptsvm) provided by Henderson and Hayes (2017). They also provide the attentional maps that can be used to evaluate the models.

Then, each image was segmented into partially overlapping circular patches at two scales: 300 patches at 3° and 108 patches at 7°. The segmentation yielded 12,000 patches at 3° and 4,320 patches at 7°, for a total of 16,320 patches. We construct data set in Q & A format to fine-tune the LLaVA model. Specifically, for patch ratings, we prompted the model to assess the meaningfulness of the input patch using the same six-point scale as human raters (set as Q): “Please assess the meaningfulness of the depicted patch using the following scale: ‘very low,’ ‘low,’ ‘somewhat low,’ ‘somewhat high,’ ‘high,’ and ‘very high.’ Provide your response by selecting one of these categories.” Please refer to "dataset.ipynb" for details.

For large-scale implementation, please refer to the file "attention_dataset.ipynb" for the steps of generating data with GPU acceleration.

After preparing the data set, we turn to finetune the LLaVA using the codes displayed in "llava-finetune.ipynb". Please note that users must first download the weights of pre-trained LLaVA from HuggingFace. We use the version of LLaVA-1.5-7B in the experiment. If parallel training is employed when finetuning the model, please ensure to merge models before evaluation.

For the inference, please follow the steps in "inference.ipynb". This would instruct the fine-tuned LLaVA model to give ratings to given patches. Then, the patch ratings are smoothed using "antonioGaussian.py" to generate meaning maps. Please refer to the "scene_compare.py" for the model evaluation.

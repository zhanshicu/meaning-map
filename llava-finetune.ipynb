{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e47084-5ae7-4691-a319-8a31e4d61478",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# This scripts follows the procedure of fine-tuning LLaVA\n",
    "\n",
    "# Users are supposed to download the pre-trained LLaVA model from HuggingFace first\n",
    "# These codes are conducted under the folder \"LLaVA\"\n",
    "# The employed LLaVA version is LLaVA-1.5-7B\n",
    "\n",
    "# Please ensure sufficient GPU storage to conduct the fine-tuning\n",
    "# 6 RTX 4090 GPUs are utilized in our experiments\n",
    "# If parallel training is used, please ensure the model is merged before evaluation\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "os.chdir(\"LLaVA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bcdefd6-f26e-4fa0-b8e0-ba26111b0976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 14:31:33,278] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/compiler_compat/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90c881f56a04ffea6ba5af749e97aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "model_path = \"llava-v1.5-7b/\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path),\n",
    "    offload_folder=\"llava_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b487fd-2a27-4de7-9f50-86513f1bfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign paths to variables\n",
    "DEEPSPEED_SCRIPT = \"deepspeed llava/train/train_xformers.py\"\n",
    "DEEPSPEED_JSON = \"./scripts/zero3.json\"\n",
    "MODEL_NAME = \"llava-v1.5-7b\"\n",
    "DATA_PATH = \"../similarity_train.json\"\n",
    "IMAGE_FOLDER = \"../survey_pairs\"\n",
    "VISION_TOWER = \"openai/clip-vit-large-patch14-336\"\n",
    "OUTPUT_DIR = \"sim_llava-v1.5-7b-lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8131d53c-7471-4647-9bfd-a3147629855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_script = f'''\n",
    "{DEEPSPEED_SCRIPT} \\\n",
    "    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\n",
    "    --deepspeed {DEEPSPEED_JSON} \\\n",
    "    --model_name_or_path {MODEL_NAME} \\\n",
    "    --version v1 \\\n",
    "    --data_path {DATA_PATH} \\\n",
    "    --image_folder {IMAGE_FOLDER} \\\n",
    "    --vision_tower {VISION_TOWER} \\\n",
    "    --mm_projector_type mlp2x_gelu \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --image_aspect_ratio pad \\\n",
    "    --group_by_modality_length True \\\n",
    "    --bf16 True \\\n",
    "    --output_dir {OUTPUT_DIR} \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 50000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --lazy_preprocess True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a48c751e-8a12-4fb3-8cfa-c0cb737f680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd279ab2-30ca-4dec-8bf6-986f1ada4e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 14:44:55,516] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "[2024-07-11 14:44:57,705] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-07-11 14:44:57,705] [INFO] [runner.py:568:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_xformers.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path llava-v1.5-7b --version v1 --data_path ../similarity_train.json --image_folder ../survey_pairs --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir sim_llava-v1.5-7b-lora --num_train_epochs 2 --per_device_train_batch_size 4 --per_device_eval_batch_size 2 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True\n",
      "[2024-07-11 14:45:00,521] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "[2024-07-11 14:45:02,523] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.9.6-1+cuda11.3\n",
      "[2024-07-11 14:45:02,523] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.9.6-1\n",
      "[2024-07-11 14:45:02,523] [INFO] [launch.py:139:main] 0 NCCL_VERSION=2.9.6-1\n",
      "[2024-07-11 14:45:02,523] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2024-07-11 14:45:02,523] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.9.6-1+cuda11.3\n",
      "[2024-07-11 14:45:02,523] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2024-07-11 14:45:02,523] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.9.6-1\n",
      "[2024-07-11 14:45:02,524] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2024-07-11 14:45:02,524] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2024-07-11 14:45:02,524] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2024-07-11 14:45:02,524] [INFO] [launch.py:164:main] dist_world_size=4\n",
      "[2024-07-11 14:45:02,524] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2024-07-11 14:45:02,540] [INFO] [launch.py:256:main] process 10899 spawned with command: ['/root/miniconda3/bin/python', '-u', 'llava/train/train_xformers.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'llava-v1.5-7b', '--version', 'v1', '--data_path', '../similarity_train.json', '--image_folder', '../survey_pairs', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', 'sim_llava-v1.5-7b-lora', '--num_train_epochs', '2', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']\n",
      "[2024-07-11 14:45:02,552] [INFO] [launch.py:256:main] process 10900 spawned with command: ['/root/miniconda3/bin/python', '-u', 'llava/train/train_xformers.py', '--local_rank=1', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'llava-v1.5-7b', '--version', 'v1', '--data_path', '../similarity_train.json', '--image_folder', '../survey_pairs', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', 'sim_llava-v1.5-7b-lora', '--num_train_epochs', '2', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']\n",
      "[2024-07-11 14:45:02,563] [INFO] [launch.py:256:main] process 10901 spawned with command: ['/root/miniconda3/bin/python', '-u', 'llava/train/train_xformers.py', '--local_rank=2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'llava-v1.5-7b', '--version', 'v1', '--data_path', '../similarity_train.json', '--image_folder', '../survey_pairs', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', 'sim_llava-v1.5-7b-lora', '--num_train_epochs', '2', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']\n",
      "[2024-07-11 14:45:02,572] [INFO] [launch.py:256:main] process 10902 spawned with command: ['/root/miniconda3/bin/python', '-u', 'llava/train/train_xformers.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'llava-v1.5-7b', '--version', 'v1', '--data_path', '../similarity_train.json', '--image_folder', '../survey_pairs', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', 'sim_llava-v1.5-7b-lora', '--num_train_epochs', '2', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']\n",
      "[2024-07-11 14:45:06,257] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-07-11 14:45:06,407] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "[2024-07-11 14:45:06,518] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "[2024-07-11 14:45:06,891] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "ERROR:root:xformers not found! Please install it before trying to use it.\n",
      "ERROR:root:xformers not found! Please install it before trying to use it.\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "ERROR:root:xformers not found! Please install it before trying to use it.\n",
      "ERROR:root:xformers not found! Please install it before trying to use it.\n",
      "[2024-07-11 14:45:08,224] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-07-11 14:45:08,445] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-07-11 14:45:08,446] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-07-11 14:45:08,578] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "[2024-07-11 14:45:09,172] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[2024-07-11 14:45:11,070] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 295, num_elems = 6.76B\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.93s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.99s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.95s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.99s/it]\n",
      "Adding LoRA adapters...\n",
      "[2024-07-11 14:45:37,925] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 686, num_elems = 7.06B\n",
      "Formatting inputs...Skip in lazy mode\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Parameter Offload: Total persistent parameters: 599040 in 312 params\n",
      "  0%|                                                    | 0/22 [00:00<?, ?it/s]/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 3.2226, 'learning_rate': 2e-05, 'epoch': 0.09}                         \n",
      "{'loss': 3.0078, 'learning_rate': 1.9888308262251286e-05, 'epoch': 0.18}        \n",
      "{'loss': 2.5674, 'learning_rate': 1.955572805786141e-05, 'epoch': 0.27}         \n",
      "{'loss': 0.9885, 'learning_rate': 1.900968867902419e-05, 'epoch': 0.36}         \n",
      "{'loss': 0.77, 'learning_rate': 1.826238774315995e-05, 'epoch': 0.45}           \n",
      "{'loss': 0.7455, 'learning_rate': 1.7330518718298263e-05, 'epoch': 0.55}        \n",
      "{'loss': 0.8256, 'learning_rate': 1.6234898018587336e-05, 'epoch': 0.64}        \n",
      "{'loss': 0.7991, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.73}        \n",
      "{'loss': 0.6492, 'learning_rate': 1.3653410243663953e-05, 'epoch': 0.82}        \n",
      "{'loss': 0.7526, 'learning_rate': 1.2225209339563144e-05, 'epoch': 0.91}        \n",
      "{'loss': 0.7219, 'learning_rate': 1.0747300935864245e-05, 'epoch': 1.0}         \n",
      "{'loss': 0.7366, 'learning_rate': 9.252699064135759e-06, 'epoch': 1.09}         \n",
      "{'loss': 0.7104, 'learning_rate': 7.774790660436857e-06, 'epoch': 1.18}         \n",
      "{'loss': 0.6824, 'learning_rate': 6.34658975633605e-06, 'epoch': 1.27}          \n",
      "{'loss': 0.662, 'learning_rate': 5.000000000000003e-06, 'epoch': 1.36}          \n",
      "{'loss': 0.6774, 'learning_rate': 3.7651019814126656e-06, 'epoch': 1.45}        \n",
      "{'loss': 0.7364, 'learning_rate': 2.669481281701739e-06, 'epoch': 1.55}         \n",
      "{'loss': 0.6795, 'learning_rate': 1.7376122568400533e-06, 'epoch': 1.64}        \n",
      "{'loss': 0.6596, 'learning_rate': 9.903113209758098e-07, 'epoch': 1.73}         \n",
      "{'loss': 0.678, 'learning_rate': 4.4427194213859216e-07, 'epoch': 1.82}         \n",
      "{'loss': 0.6692, 'learning_rate': 1.1169173774871478e-07, 'epoch': 1.91}        \n",
      "{'loss': 0.6934, 'learning_rate': 0.0, 'epoch': 2.0}                            \n",
      "{'train_runtime': 79.1912, 'train_samples_per_second': 4.445, 'train_steps_per_second': 0.278, 'train_loss': 1.0288681929761714, 'epoch': 2.0}\n",
      "100%|███████████████████████████████████████████| 22/22 [01:19<00:00,  3.60s/it]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 4096}\n",
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /llava-v1.5-7b/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7efc36f0caf0>: Failed to establish a new connection: [Errno 111] Connection refused'))\"), '(Request ID: b72a9760-e1f2-4761-9a12-002600a48f92)') - silently ignoring the lookup for the file config.json in llava-v1.5-7b.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in llava-v1.5-7b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[2024-07-11 14:47:24,778] [INFO] [launch.py:351:main] Process 10901 exits successfully.\n",
      "[2024-07-11 14:47:24,778] [INFO] [launch.py:351:main] Process 10902 exits successfully.\n",
      "[2024-07-11 14:47:25,780] [INFO] [launch.py:351:main] Process 10899 exits successfully.\n",
      "[2024-07-11 14:47:25,780] [INFO] [launch.py:351:main] Process 10900 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!{finetune_script}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4acb6-8627-4a59-9382-1a2469b88a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Reproduction on COCO Dataset\n",
    "\n",
    "Pipeline Steps:\n",
    "1. **Data Generation**: Download COCO images and generate teacher meaning maps (using LLaVA).\n",
    "2. **Training**: Train the U-Net on these (Image, Meaning Map) pairs.\n",
    "3. **Evaluation**: Evaluate the U-Net on the 14 held-out Henderson & Hayes test scenes.\n",
    "\n",
    "Prerequisites\n",
    "Ensure you have run `utils/generate_public_training_data.py` and the LLaVA inference pipeline to populate `public_training_data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(512, 1024))\n",
    "        self.down5 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(1024, 1024))\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.up_conv1 = DoubleConv(1024, 512)\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up_conv2 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up_conv3 = DoubleConv(256, 128)\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up_conv4 = DoubleConv(128, 64)\n",
    "        self.up5 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
    "        self.up_conv5 = DoubleConv(128, 64)\n",
    "\n",
    "        # Feature reduction layers (Custom Architecture)\n",
    "        self.reduce1 = nn.Sequential(nn.Conv2d(1024, 768, kernel_size=1), nn.ReLU(inplace=True), nn.Conv2d(768, 512, kernel_size=1))\n",
    "        self.reduce2 = nn.Sequential(nn.Conv2d(512, 384, kernel_size=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=1))\n",
    "        self.reduce3 = nn.Sequential(nn.Conv2d(256, 192, kernel_size=1), nn.ReLU(inplace=True), nn.Conv2d(192, 128, kernel_size=1))\n",
    "        self.reduce4 = nn.Sequential(nn.Conv2d(128, 96, kernel_size=1), nn.ReLU(inplace=True), nn.Conv2d(96, 64, kernel_size=1))\n",
    "        self.reduce5 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1))\n",
    "\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.outc = nn.Conv2d(16, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x6 = self.down5(x5)\n",
    "\n",
    "        x = self.up1(x6)\n",
    "        x5_reduced = self.reduce1(x5)\n",
    "        x = self.up_conv1(torch.cat([x, x5_reduced], dim=1))\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x4_reduced = self.reduce2(x4)\n",
    "        x = self.up_conv2(torch.cat([x, x4_reduced], dim=1))\n",
    "\n",
    "        x = self.up3(x)\n",
    "        x3_reduced = self.reduce3(x3)\n",
    "        x = self.up_conv3(torch.cat([x, x3_reduced], dim=1))\n",
    "\n",
    "        x = self.up4(x)\n",
    "        x2_reduced = self.reduce4(x2)\n",
    "        x = self.up_conv4(torch.cat([x, x2_reduced], dim=1))\n",
    "\n",
    "        x = self.up5(x)\n",
    "        x1_reduced = self.reduce5(x1)\n",
    "        x = self.up_conv5(torch.cat([x, x1_reduced], dim=1))\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading\n",
    "Loads (Image, Meaning Map) pairs from the `public_training_data` directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODistillationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.image_dir = os.path.join(root_dir, 'images')\n",
    "        self.map_dir = os.path.join(root_dir, 'meaning_maps')\n",
    "        \n",
    "        # List all images\n",
    "        self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png'))])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        # Assume map has same basename\n",
    "        basename = os.path.splitext(img_name)[0]\n",
    "        # Check for map extension (npy, jpg, png)\n",
    "        map_name = None\n",
    "        for ext in ['.npy', '.jpg', '.png']:\n",
    "            if os.path.exists(os.path.join(self.map_dir, basename + ext)):\n",
    "                map_name = basename + ext\n",
    "                break\n",
    "        \n",
    "        if map_name is None:\n",
    "             # Fallback: return zero map if missing (or handle error)\n",
    "             # For robustness in training initialization\n",
    "             print(f\"Warning: Map not found for {img_name}\")\n",
    "             map_name = basename + '.png' \n",
    "\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        map_path = os.path.join(self.map_dir, map_name)\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Load target map\n",
    "        if map_path.endswith('.npy'):\n",
    "            target_map = np.load(map_path)\n",
    "            target_map = Image.fromarray((target_map * 255).astype(np.uint8))\n",
    "        else:\n",
    "            # If image file exists\n",
    "            if os.path.exists(map_path):\n",
    "               target_map = Image.open(map_path).convert('L')\n",
    "            else:\n",
    "               target_map = Image.new('L', image.size)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            target_map = self.target_transform(target_map)\n",
    "        \n",
    "        return image, target_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop\n",
    "Trains the model for 20 epochs (default) using MSE Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir='public_training_data', batch_size=8, num_epochs=20, lr=1e-4):\n",
    "    # Transforms\n",
    "    # Resize to 256x256 for training efficiency\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    map_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset = COCODistillationDataset(data_dir, transform=img_transform, target_transform=map_transform)\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"No data found! Please generate data first.\")\n",
    "        return\n",
    "\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    model = UNet(n_channels=3, n_classes=1).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(torch.sigmoid(outputs), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(torch.sigmoid(outputs), targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}')\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'unet_coco_best.pth')\n",
    "            print(\"Saved best model.\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation on Henderson & Hayes Scenes\n",
    "Evaluates the trained U-Net on the 14 held-out scenes to reproduce the correlation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test_scenes(model_path='unet_coco_best.pth', test_dir='scenes', attention_dir='attention_maps'):\n",
    "    model = UNet(n_channels=3, n_classes=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)), # Inference size\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    test_images = sorted(glob(os.path.join(test_dir, '*.jpg')))\n",
    "    \n",
    "    correlations = []\n",
    "    \n",
    "    print(f\"\\nEvaluating on {len(test_images)} test scenes...\")\n",
    "    \n",
    "    for img_path in test_images:\n",
    "        scene_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        \n",
    "        # Load Image\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        original_size = img.size\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "            pred_map = torch.sigmoid(output).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Resize prediction back to original size for correlation\n",
    "        pred_map_pil = Image.fromarray((pred_map * 255).astype(np.uint8))\n",
    "        pred_map_resized = pred_map_pil.resize(original_size, Image.BICUBIC)\n",
    "        pred_flat = np.array(pred_map_resized).flatten().astype(float)\n",
    "        \n",
    "        # Load Ground Truth Attention Map (if available)\n",
    "        # Typically in meaning-map/result/attention_maps\n",
    "        # Try several patterns\n",
    "        possible_gt_paths = [\n",
    "            os.path.join(attention_dir, f\"{scene_id}.png\"),\n",
    "            os.path.join(attention_dir, f\"{scene_id}.jpg\"),\n",
    "            os.path.join(attention_dir, f\"scene_{scene_id}.png\") # Common internal naming\n",
    "        ]\n",
    "        \n",
    "        gt_path = None\n",
    "        for p in possible_gt_paths:\n",
    "            if os.path.exists(p):\n",
    "                gt_path = p\n",
    "                break\n",
    "        \n",
    "        if gt_path:\n",
    "            gt_map = Image.open(gt_path).convert('L')\n",
    "            gt_map = gt_map.resize(original_size, Image.BICUBIC)\n",
    "            gt_flat = np.array(gt_map).flatten().astype(float)\n",
    "            \n",
    "            corr, _ = pearsonr(pred_flat, gt_flat)\n",
    "            correlations.append(corr)\n",
    "            print(f\"{scene_id}: r = {corr:.3f}\")\n",
    "        else:\n",
    "            print(f\"{scene_id}: GT Attention Map not found in {attention_dir}\")\n",
    "            \n",
    "    if correlations:\n",
    "        print(f\"\\nMean Test Correlation: {np.mean(correlations):.3f} (SD={np.std(correlations):.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
